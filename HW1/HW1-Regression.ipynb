{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"Reading data...\"\n",
    "data = list(parseData(\"file:beer_50000.json\"))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tripel': 257, 'American Black Ale': 138, 'Fruit / Vegetable Beer': 1355, 'English Brown Ale': 495, 'American Pale Ale (APA)': 2288, 'American Amber / Red Lager': 42, 'Chile Beer': 11, 'Cream Ale': 69, 'Dubbel': 165, 'English Dark Mild Ale': 21, 'Belgian Strong Pale Ale': 632, 'Weizenbock': 13, 'Milk / Sweet Stout': 69, 'Dunkelweizen': 61, 'Belgian Strong Dark Ale': 146, 'Foreign / Export Stout': 55, 'Low Alcohol Beer': 7, 'Euro Pale Lager': 701, 'Schwarzbier': 53, 'Munich Dunkel Lager': 141, 'Maibock / Helles Bock': 225, 'Belgian Pale Ale': 144, 'Irish Dry Stout': 101, 'M\\xc3\\xa4rzen / Oktoberfest': 557, 'California Common / Steam Beer': 11, 'American Porter': 2230, 'Vienna Lager': 33, 'Oatmeal Stout': 102, 'Bi\\xc3\\xa8re de Garde': 7, 'Doppelbock': 873, 'American Adjunct Lager': 242, 'Wheatwine': 455, 'Scottish Ale': 78, 'English Porter': 367, 'Saison / Farmhouse Ale': 141, 'Altbier': 165, 'English India Pale Ale (IPA)': 175, 'Dortmunder / Export Lager': 31, 'English Bitter': 267, 'Smoked Beer': 61, 'Lambic - Fruit': 6, 'Munich Helles Lager': 650, 'American Barleywine': 825, 'English Pale Ale': 1324, 'Belgian Dark Ale': 175, 'Rauchbier': 1938, 'Hefeweizen': 618, 'Euro Strong Lager': 329, 'Baltic Porter': 514, 'Pumpkin Ale': 560, 'Herbed / Spiced Beer': 73, 'Bock': 148, 'Euro Dark Lager': 144, 'American IPA': 4113, 'Lambic - Unblended': 10, 'English Barleywine': 133, 'American Dark Wheat Ale': 14, 'Keller Bier / Zwickel Bier': 23, 'American Strong Ale': 166, 'English Stout': 136, 'American Pale Lager': 123, 'Belgian IPA': 128, 'American Brown Ale': 314, 'Braggot': 26, 'American Double / Imperial IPA': 3886, 'American Wild Ale': 98, 'English Pale Mild Ale': 21, 'Quadrupel (Quad)': 119, 'Light Lager': 503, 'Russian Imperial Stout': 2695, 'Flanders Oud Bruin': 13, 'Berliner Weissbier': 10, 'Scottish Gruit / Ancient Herbed Ale': 65, 'Extra Special / Strong Bitter (ESB)': 667, 'American Double / Imperial Stout': 5964, 'Scotch Ale / Wee Heavy': 2776, 'Winter Warmer': 259, 'Old Ale': 1052, 'German Pilsener': 586, 'K\\xc3\\xb6lsch': 94, 'Black & Tan': 122, 'Flanders Red Ale': 2, 'American Malt Liquor': 90, 'Czech Pilsener': 1501, 'Kristalweizen': 7, 'American Amber / Red Ale': 665, 'American Pale Wheat Ale': 154, 'Rye Beer': 1798, 'English Strong Ale': 164, 'American Blonde Ale': 357, 'Witbier': 162, 'American Stout': 591, 'Irish Red Ale': 83, 'Eisbock': 8, 'American Double / Imperial Pilsner': 14}\n"
     ]
    }
   ],
   "source": [
    "style_num = {}\n",
    "review_taste_total = {}\n",
    "review_taste_average = {}\n",
    "\n",
    "for single_data in data:\n",
    "    if style_num.has_key(single_data['beer/style']):\n",
    "        style_num[single_data['beer/style']] += 1\n",
    "        review_taste_total[single_data['beer/style']] += single_data['review/taste']\n",
    "    else:\n",
    "        style_num[single_data['beer/style']] = 1\n",
    "        review_taste_total[single_data['beer/style']] = single_data['review/taste']\n",
    "# style_num stores the answer for tasks-regression problem 1\n",
    "print style_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tripel': 3.7840466926070038, 'American Black Ale': 3.8731884057971016, 'Fruit / Vegetable Beer': 3.607749077490775, 'English Brown Ale': 3.728282828282828, 'American Pale Ale (APA)': 3.649694055944056, 'American Amber / Red Lager': 3.6904761904761907, 'Chile Beer': 3.9545454545454546, 'Cream Ale': 3.028985507246377, 'Dubbel': 3.7363636363636363, 'English Dark Mild Ale': 3.7857142857142856, 'Belgian Strong Pale Ale': 4.056170886075949, 'Weizenbock': 3.3846153846153846, 'Milk / Sweet Stout': 3.782608695652174, 'Dunkelweizen': 3.4918032786885247, 'Belgian Strong Dark Ale': 3.6952054794520546, 'Foreign / Export Stout': 3.2545454545454544, 'American Porter': 4.081838565022421, 'American Double / Imperial Pilsner': 3.8214285714285716, 'Euro Pale Lager': 2.962910128388017, 'Pumpkin Ale': 3.7875, 'Munich Dunkel Lager': 3.780141843971631, 'Maibock / Helles Bock': 3.7466666666666666, 'Belgian Pale Ale': 3.7395833333333335, 'Irish Dry Stout': 3.623762376237624, 'M\\xc3\\xa4rzen / Oktoberfest': 3.5933572710951527, 'California Common / Steam Beer': 3.3181818181818183, 'Vienna Lager': 3.5303030303030303, 'Oatmeal Stout': 3.7745098039215685, 'Bi\\xc3\\xa8re de Garde': 3.9285714285714284, 'Doppelbock': 3.9828178694158076, 'American Adjunct Lager': 2.9483471074380163, 'Wheatwine': 4.186813186813187, 'Scottish Ale': 3.7628205128205128, 'English Porter': 3.70708446866485, 'Saison / Farmhouse Ale': 3.702127659574468, 'Altbier': 3.403030303030303, 'English India Pale Ale (IPA)': 3.4714285714285715, 'Low Alcohol Beer': 2.7142857142857144, 'Dortmunder / Export Lager': 3.4193548387096775, 'English Bitter': 3.5374531835205993, 'Smoked Beer': 3.19672131147541, 'Lambic - Fruit': 3.75, 'Munich Helles Lager': 3.959230769230769, 'American Barleywine': 4.064242424242424, 'Baltic Porter': 4.213035019455253, 'Belgian Dark Ale': 3.34, 'Rauchbier': 4.067853457172343, 'American Strong Ale': 3.569277108433735, 'Euro Strong Lager': 2.8480243161094223, 'English Pale Ale': 3.483761329305136, 'Herbed / Spiced Beer': 3.4452054794520546, 'Bock': 3.189189189189189, 'Euro Dark Lager': 3.704861111111111, 'American IPA': 4.00085096036956, 'Lambic - Unblended': 3.3, 'English Barleywine': 4.360902255639098, 'American Dark Wheat Ale': 3.6785714285714284, 'Keller Bier / Zwickel Bier': 3.869565217391304, 'Hefeweizen': 3.635113268608414, 'English Stout': 3.599264705882353, 'American Pale Lager': 3.2154471544715446, 'Belgian IPA': 3.94921875, 'American Brown Ale': 3.7436305732484074, 'Braggot': 3.8076923076923075, 'American Double / Imperial IPA': 4.033324755532681, 'Irish Red Ale': 2.9819277108433737, 'American Wild Ale': 4.188775510204081, 'English Pale Mild Ale': 3.5952380952380953, 'Quadrupel (Quad)': 3.596638655462185, 'Light Lager': 2.39662027833002, 'Russian Imperial Stout': 4.300371057513915, 'Flanders Oud Bruin': 3.923076923076923, 'Berliner Weissbier': 3.55, 'Scottish Gruit / Ancient Herbed Ale': 3.9076923076923076, 'Extra Special / Strong Bitter (ESB)': 3.685157421289355, 'American Double / Imperial Stout': 4.479963112005366, 'Scotch Ale / Wee Heavy': 4.083393371757925, 'Winter Warmer': 3.6216216216216215, 'Old Ale': 4.096007604562738, 'German Pilsener': 3.667235494880546, 'K\\xc3\\xb6lsch': 3.6968085106382977, 'Black & Tan': 3.942622950819672, 'Flanders Red Ale': 3.25, 'American Malt Liquor': 2.2555555555555555, 'Schwarzbier': 3.6226415094339623, 'Kristalweizen': 2.7857142857142856, 'American Amber / Red Ale': 3.513533834586466, 'American Pale Wheat Ale': 3.3344155844155843, 'Rye Beer': 4.213570634037819, 'English Strong Ale': 3.7560975609756095, 'American Blonde Ale': 3.2549019607843137, 'American Stout': 3.8197969543147208, 'Czech Pilsener': 3.609593604263824, 'Eisbock': 3.75, 'Witbier': 3.5277777777777777}\n"
     ]
    }
   ],
   "source": [
    "for style in review_taste_total.keys():\n",
    "    review_taste_average[style] = review_taste_total[style] / style_num[style]\n",
    "# review_taste_average stores the answer for tasks-regression problem 1   \n",
    "print review_taste_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "  if datum['beer/style'] == 'American IPA':\n",
    "    feat = [1,1]\n",
    "  else:\n",
    "    feat = [1,0]\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/taste'] for d in data]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset = [[0.5130161734617561]]\n",
      "gradient = [ -1.08186669e-13  -1.64709490e-14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.91520474,  0.08564622]),\n",
       " [0.5130161734617561],\n",
       " {'funcalls': 1,\n",
       "  'grad': array([ -1.08186669e-13,  -1.64709490e-14]),\n",
       "  'nit': 0,\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Gradient descent ###\n",
    "# Objective\n",
    "def f(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  diffSq = diff.T*diff\n",
    "  diffSqReg = diffSq / len(X) + lam*(theta.T*theta)\n",
    "  print \"offset =\", diffSqReg.flatten().tolist()\n",
    "  return diffSqReg.flatten().tolist()[0]\n",
    "# Derivative\n",
    "def fprime(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  res = 2*X.T*diff / len(X) + 2*lam*theta\n",
    "  print \"gradient =\", numpy.array(res.flatten().tolist()[0])\n",
    "  return numpy.array(res.flatten().tolist()[0])\n",
    "\n",
    "scipy.optimize.fmin_l_bfgs_b(f, [theta[0],theta[1]], fprime, args = (X, y, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset = [[0.5130161734617561]]\n",
      "gradient = [ -1.08186669e-13  -1.64709490e-14]\n",
      "[ 3.91520474  0.08564622]\n"
     ]
    }
   ],
   "source": [
    "theta = scipy.optimize.fmin_l_bfgs_b(f, [theta[0],theta[1]], fprime, args = (X, y, 0))[0]\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:int(len(data)/2)]\n",
    "test_data = data[int(len(data)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset = [[0.5581072865586575]]\n",
      "gradient = [ -8.32005753e-14  -5.42968337e-15]\n"
     ]
    }
   ],
   "source": [
    "def feature(datum):\n",
    "  if datum['beer/style'] == 'American IPA':\n",
    "    feat = [1,1]\n",
    "  else:\n",
    "    feat = [1,0]\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in train_data]\n",
    "y = [d['review/taste'] for d in train_data]\n",
    "newtheta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "### Gradient descent ###\n",
    "# Objective\n",
    "def f(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  diffSq = diff.T*diff\n",
    "  diffSqReg = diffSq / len(X) + lam*(theta.T*theta)\n",
    "  print \"offset =\", diffSqReg.flatten().tolist()\n",
    "  return diffSqReg.flatten().tolist()[0]\n",
    "# Derivative\n",
    "def fprime(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  res = 2*X.T*diff / len(X) + 2*lam*theta\n",
    "  print \"gradient =\", numpy.array(res.flatten().tolist()[0])\n",
    "  return numpy.array(res.flatten().tolist()[0])\n",
    "\n",
    "theta = scipy.optimize.fmin_l_bfgs_b(f, [newtheta[0],newtheta[1]], fprime, args = (X, y, 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset = [[0.5581072865586575]]\n",
      "gradient = [ -8.32005753e-14  -5.42968337e-15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.90435639,  0.05606027]),\n",
       " [0.5581072865586575],\n",
       " {'funcalls': 1,\n",
       "  'grad': array([ -8.32005753e-14,  -5.42968337e-15]),\n",
       "  'nit': 0,\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(f, [newtheta[0],newtheta[1]], fprime, args = (X, y, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46841005]]\n",
      "[[ 0.55810729]]\n"
     ]
    }
   ],
   "source": [
    "def feature(datum):\n",
    "  if datum['beer/style'] == 'American IPA':\n",
    "    feat = [1,1]\n",
    "  else:\n",
    "    feat = [1,0]\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in test_data]\n",
    "y = [d['review/taste'] for d in test_data]\n",
    "count = 0\n",
    "sum_Diff_square = 0\n",
    "for testData in test_data:\n",
    "    sum_Diff_square += (y[count] - theta * numpy.matrix(X[count]).T) ** 2\n",
    "    count += 1\n",
    "test_MSE = sum_Diff_square / count\n",
    "print test_MSE\n",
    "\n",
    "X = [feature(d) for d in train_data]\n",
    "y = [d['review/taste'] for d in train_data]\n",
    "count = 0\n",
    "sum_Diff_square = 0\n",
    "for trainData in train_data:\n",
    "    sum_Diff_square += (y[count] - theta * numpy.matrix(X[count]).T) ** 2\n",
    "    count += 1\n",
    "train_MSE = sum_Diff_square / count\n",
    "print train_MSE\n",
    "\n",
    "# MSE is the answer for problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "style_50more = {}\n",
    "for style in style_num.keys():\n",
    "    if style_num[style] >= 50:\n",
    "        style_50more[style] = style_num[style]\n",
    "\n",
    "print len(style_num)\n",
    "print len(style_50more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "theta =  [ 3.60681818  0.05895722 -0.50681818 -0.10681818 -0.62765152  0.19621212\n",
      "  0.35359848  0.69564175  0.16385851 -0.18884943 -1.00681818  0.22651515\n",
      " -0.22045455 -0.45410882  0.02739474  0.76540404  0.3449362   0.38356643\n",
      " -0.10223103  0.8416167   0.48544372 -0.41285266 -1.23390152 -0.73802386\n",
      "  0.01818182  0.60472028  0.26818182  0.09815076 -0.2937747  -0.1798951\n",
      "  0.20984848 -0.65227273  0.11320382  0.10049889 -0.13106061 -0.58598485\n",
      "  0.26761104  0.33596789  0.20312334  0.01699134  0.33580477  0.13786267\n",
      "  0.1527972  -0.02450111 -0.4664673   0.46842454 -0.20681818  0.45638407\n",
      "  0.03420746 -0.31931818  0.12193999  0.25681818 -0.27061129 -0.63806818\n",
      "  0.12651515  0.44318182  0.71136364 -0.0058248  -0.11634199 -0.83277972\n",
      "  0.65508658 -0.35681818 -0.17824675 -0.38622995  0.3763751  -0.74318182\n",
      "  0.11433566  0.3305986   0.58195733 -0.09003966 -0.92019846  0.12395105\n",
      "  0.26709486  0.39318182  0.14466111]\n",
      "[ 9196.00692725]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for single_data in train_data:\n",
    "    X_tmp = [1]\n",
    "    for style in style_50more:\n",
    "        def feature(datum):\n",
    "            if datum['beer/style'] == style:\n",
    "                feat = [1]\n",
    "            else:\n",
    "                feat = [0]\n",
    "            return feat\n",
    "        X_tmp += feature(single_data)\n",
    "    X.append(X_tmp)\n",
    "y = [d['review/taste'] for d in train_data]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "print len(theta)\n",
    "print 'theta = ', theta\n",
    "print residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset = [[0.36784027709014366]]\n",
      "gradient = [  1.20053301e-14  -3.99609235e-16  -1.05728759e-16  -3.19744231e-18\n",
      "   1.84741111e-16  -7.23616722e-16  -1.20121513e-14   2.28027375e-14\n",
      "   2.20268248e-18   6.36646291e-17   4.02167188e-17  -1.05444542e-16\n",
      "  -1.49675827e-16  -6.92352842e-16   3.91366939e-16  -6.04245542e-16\n",
      "  -1.58308922e-16   2.75406364e-16  -5.54223334e-17  -6.86668500e-16\n",
      "   4.59522198e-15   5.22959454e-17   9.76214665e-16   2.40447662e-16\n",
      "  -8.81072992e-18  -9.57811608e-17   4.53681537e-17   1.44382284e-16\n",
      "   8.14281975e-17   3.16902060e-17  -1.32160949e-17  -5.73407988e-17\n",
      "   3.25144356e-16  -2.30215846e-17   1.97246663e-16  -1.71667125e-16\n",
      "  -3.54134500e-16  -2.19529284e-15   5.88187277e-16   2.58069122e-16\n",
      "  -6.73239242e-17   4.06430445e-17   1.11768372e-16   2.19912977e-16\n",
      "  -5.18696197e-18   7.61701813e-16  -5.36459765e-18   5.91171556e-17\n",
      "   1.04591891e-16   1.50066626e-16   2.50963694e-16   4.01456646e-18\n",
      "  -1.90709670e-16   3.63797881e-16   8.01492206e-17  -1.00186526e-16\n",
      "  -2.95585778e-17   3.83550969e-16   4.74500439e-16   7.45501438e-16\n",
      "  -4.03588274e-17   3.34665629e-17   8.10018719e-18   1.23634436e-17\n",
      "  -2.87059265e-17   1.07434062e-16   3.33955086e-17   4.53326265e-17\n",
      "   1.16529009e-17   2.08544293e-17   2.12025952e-16   3.15480975e-17\n",
      "   1.70530257e-18   1.91846539e-18   2.74269496e-17]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 3.60681818,  0.05895722, -0.50681818, -0.10681818, -0.62765152,\n",
       "         0.19621212,  0.35359848,  0.69564175,  0.16385851, -0.18884943,\n",
       "        -1.00681818,  0.22651515, -0.22045455, -0.45410882,  0.02739474,\n",
       "         0.76540404,  0.3449362 ,  0.38356643, -0.10223103,  0.8416167 ,\n",
       "         0.48544372, -0.41285266, -1.23390152, -0.73802386,  0.01818182,\n",
       "         0.60472028,  0.26818182,  0.09815076, -0.2937747 , -0.1798951 ,\n",
       "         0.20984848, -0.65227273,  0.11320382,  0.10049889, -0.13106061,\n",
       "        -0.58598485,  0.26761104,  0.33596789,  0.20312334,  0.01699134,\n",
       "         0.33580477,  0.13786267,  0.1527972 , -0.02450111, -0.4664673 ,\n",
       "         0.46842454, -0.20681818,  0.45638407,  0.03420746, -0.31931818,\n",
       "         0.12193999,  0.25681818, -0.27061129, -0.63806818,  0.12651515,\n",
       "         0.44318182,  0.71136364, -0.0058248 , -0.11634199, -0.83277972,\n",
       "         0.65508658, -0.35681818, -0.17824675, -0.38622995,  0.3763751 ,\n",
       "        -0.74318182,  0.11433566,  0.3305986 ,  0.58195733, -0.09003966,\n",
       "        -0.92019846,  0.12395105,  0.26709486,  0.39318182,  0.14466111]),\n",
       " [0.36784027709014366],\n",
       " {'funcalls': 1,\n",
       "  'grad': array([  1.20053301e-14,  -3.99609235e-16,  -1.05728759e-16,\n",
       "          -3.19744231e-18,   1.84741111e-16,  -7.23616722e-16,\n",
       "          -1.20121513e-14,   2.28027375e-14,   2.20268248e-18,\n",
       "           6.36646291e-17,   4.02167188e-17,  -1.05444542e-16,\n",
       "          -1.49675827e-16,  -6.92352842e-16,   3.91366939e-16,\n",
       "          -6.04245542e-16,  -1.58308922e-16,   2.75406364e-16,\n",
       "          -5.54223334e-17,  -6.86668500e-16,   4.59522198e-15,\n",
       "           5.22959454e-17,   9.76214665e-16,   2.40447662e-16,\n",
       "          -8.81072992e-18,  -9.57811608e-17,   4.53681537e-17,\n",
       "           1.44382284e-16,   8.14281975e-17,   3.16902060e-17,\n",
       "          -1.32160949e-17,  -5.73407988e-17,   3.25144356e-16,\n",
       "          -2.30215846e-17,   1.97246663e-16,  -1.71667125e-16,\n",
       "          -3.54134500e-16,  -2.19529284e-15,   5.88187277e-16,\n",
       "           2.58069122e-16,  -6.73239242e-17,   4.06430445e-17,\n",
       "           1.11768372e-16,   2.19912977e-16,  -5.18696197e-18,\n",
       "           7.61701813e-16,  -5.36459765e-18,   5.91171556e-17,\n",
       "           1.04591891e-16,   1.50066626e-16,   2.50963694e-16,\n",
       "           4.01456646e-18,  -1.90709670e-16,   3.63797881e-16,\n",
       "           8.01492206e-17,  -1.00186526e-16,  -2.95585778e-17,\n",
       "           3.83550969e-16,   4.74500439e-16,   7.45501438e-16,\n",
       "          -4.03588274e-17,   3.34665629e-17,   8.10018719e-18,\n",
       "           1.23634436e-17,  -2.87059265e-17,   1.07434062e-16,\n",
       "           3.33955086e-17,   4.53326265e-17,   1.16529009e-17,\n",
       "           2.08544293e-17,   2.12025952e-16,   3.15480975e-17,\n",
       "           1.70530257e-18,   1.91846539e-18,   2.74269496e-17]),\n",
       "  'nit': 0,\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Gradient descent ###\n",
    "# Objective\n",
    "def f(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  diffSq = diff.T*diff\n",
    "  diffSqReg = diffSq / len(X) + lam*(theta.T*theta)\n",
    "  print \"offset =\", diffSqReg.flatten().tolist()\n",
    "  return diffSqReg.flatten().tolist()[0]\n",
    "# Derivative\n",
    "def fprime(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  res = 2*X.T*diff / len(X) + 2*lam*theta\n",
    "  print \"gradient =\", numpy.array(res.flatten().tolist()[0])\n",
    "  return numpy.array(res.flatten().tolist()[0])\n",
    "\n",
    "scipy.optimize.fmin_l_bfgs_b(f, theta, fprime, args = (X, y, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset = [[0.36784027709014366]]\n",
      "gradient = [  1.23145583e-14  -3.93640676e-16  -1.04307674e-16  -1.27897692e-18\n",
      "   1.73656645e-16  -7.40385531e-16  -1.20689947e-14   2.28027375e-14\n",
      "  -1.09423581e-17   5.74473802e-17   4.37694325e-17  -1.05444542e-16\n",
      "  -1.41682222e-16  -6.64499566e-16   3.83693077e-16  -5.91455773e-16\n",
      "  -1.50208734e-16   2.66169309e-16  -5.84421400e-17  -6.86668500e-16\n",
      "   4.59749572e-15   6.16751095e-17   1.02772901e-15   2.53805865e-16\n",
      "  -7.38964445e-18  -9.57811608e-17   4.68958206e-17   1.44382284e-16\n",
      "   9.48929824e-17   2.91322522e-17  -2.60058641e-17  -5.60618219e-17\n",
      "   3.80850906e-16  -1.53477231e-17   1.73798753e-16  -1.78488335e-16\n",
      "  -3.56976670e-16  -2.04749995e-15   6.00266503e-16   2.44426701e-16\n",
      "  -6.73239242e-17   3.21165317e-17   1.05941922e-16   1.92770244e-16\n",
      "  -1.42108547e-18   7.61701813e-16  -4.83169060e-18  -6.42330633e-17\n",
      "   9.35074240e-17   1.33013600e-16   2.33626452e-16   8.88178420e-19\n",
      "  -1.72235559e-16   3.63797881e-16   6.77857770e-17  -1.03739239e-16\n",
      "  -2.95585778e-17   3.69198006e-16   3.99325017e-16   7.45501438e-16\n",
      "  -4.33431069e-17   3.37507799e-17   1.02318154e-17   1.23634436e-17\n",
      "  -2.87059265e-17   1.06155085e-16   3.55271368e-17   2.01794137e-17\n",
      "  -2.27373675e-18   3.25428573e-17   1.97530881e-16   3.33955086e-17\n",
      "   1.70530257e-18   1.49213975e-18   2.74269496e-17]\n",
      "[[ 0.43366951]]\n"
     ]
    }
   ],
   "source": [
    "theta = scipy.optimize.fmin_l_bfgs_b(f, theta, fprime, args = (X, y, 0))[0]\n",
    "X = []\n",
    "for single_data in test_data:\n",
    "    X_tmp = [1]\n",
    "    for style in style_50more:\n",
    "        def feature(datum):\n",
    "            if datum['beer/style'] == style:\n",
    "                feat = [1]\n",
    "            else:\n",
    "                feat = [0]\n",
    "            return feat\n",
    "        X_tmp += feature(single_data)\n",
    "    X.append(X_tmp)\n",
    "y = [d['review/taste'] for d in test_data]\n",
    "count = 0\n",
    "sum_Diff_square = 0\n",
    "for testData in test_data:\n",
    "    sum_Diff_square += (y[count] - theta * numpy.matrix(X[count]).T) ** 2\n",
    "    count += 1\n",
    "test_MSE = sum_Diff_square / count\n",
    "print test_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n",
      "offset = [[15.730542077145518]]\n",
      "gradient = [-7.8098809  -0.12091359]\n",
      "offset = [[9.019956530887699]]\n",
      "gradient = [-5.60967714 -0.08716214]\n",
      "offset = [[1.8678065242136093]]\n",
      "gradient = [  1.53716340e-05  -9.92863150e-04]\n",
      "offset = [[1.8678060995104089]]\n",
      "gradient = [  1.39425607e-05  -8.89205217e-04]\n",
      "offset = [[1.8678043781773557]]\n",
      "gradient = [  2.44138043e-13   8.50014503e-17]\n"
     ]
    }
   ],
   "source": [
    "# %load week1\n",
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "\n",
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print \"Reading data...\"\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print \"done\"\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1]\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "### Convince ourselves that basic linear algebra operations yield the same answer ###\n",
    "\n",
    "X = numpy.matrix(X)\n",
    "y = numpy.matrix(y)\n",
    "numpy.linalg.inv(X.T * X) * X.T * y.T\n",
    "\n",
    "### Do older people rate beer more highly? ###\n",
    "\n",
    "data2 = [d for d in data if d.has_key('user/ageInSeconds')]\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1]\n",
    "  feat.append(datum['user/ageInSeconds'])\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data2]\n",
    "y = [d['review/overall'] for d in data2]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "### How much do women prefer beer over men? ###\n",
    "\n",
    "data2 = [d for d in data if d.has_key('user/gender')]\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1]\n",
    "  if datum['user/gender'] == \"Male\":\n",
    "    feat.append(0)\n",
    "  else:\n",
    "    feat.append(1)\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data2]\n",
    "y = [d['review/overall'] for d in data2]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "### Gradient descent ###\n",
    "\n",
    "# Objective\n",
    "def f(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  diffSq = diff.T*diff\n",
    "  diffSqReg = diffSq / len(X) + lam*(theta.T*theta)\n",
    "  print \"offset =\", diffSqReg.flatten().tolist()\n",
    "  return diffSqReg.flatten().tolist()[0]\n",
    "\n",
    "# Derivative\n",
    "def fprime(theta, X, y, lam):\n",
    "  theta = numpy.matrix(theta).T\n",
    "  X = numpy.matrix(X)\n",
    "  y = numpy.matrix(y).T\n",
    "  diff = X*theta - y\n",
    "  res = 2*X.T*diff / len(X) + 2*lam*theta\n",
    "  print \"gradient =\", numpy.array(res.flatten().tolist()[0])\n",
    "  return numpy.array(res.flatten().tolist()[0])\n",
    "\n",
    "scipy.optimize.fmin_l_bfgs_b(f, [0,0], fprime, args = (X, y, 0.1))\n",
    "\n",
    "### Random features ###\n",
    "\n",
    "def feature(datum):\n",
    "  return [random.random() for x in range(30)]\n",
    "\n",
    "X = [feature(d) for d in data2]\n",
    "y = [d['review/overall'] for d in data2]\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
